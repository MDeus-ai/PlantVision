{
 "cells": [
  {
   "cell_type": "code",
   "id": "7175a54cea3df1bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:36:01.264143Z",
     "start_time": "2025-08-11T08:35:50.182772Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from PlantVision import paths"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:36:01.547790Z",
     "start_time": "2025-08-11T08:36:01.294211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DATA PIPELINE\n",
    "def get_dataloader(data_location: Path, img_size: int, batch_size: int=5, shuffle:bool=True):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create a dataset\n",
    "    dataset = datasets.ImageFolder(root=data_location, transform=transform)\n",
    "\n",
    "    # For loading the datasets iteratively\n",
    "    return DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=1,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "loader = get_dataloader(data_location=paths.DATA_DIR / \"processed\" / \"train\", img_size=224, batch_size=10)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:36:06.780258Z",
     "start_time": "2025-08-11T08:36:02.173832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img, lab = next(iter(loader))\n",
    "img.shape"
   ],
   "id": "4968c05a500ddd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 224, 224])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:36:06.850479Z",
     "start_time": "2025-08-11T08:36:06.842586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODEL DEFINITION\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Feature Extraction\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, stride=1, padding=1) # --> output (10, 224, 224)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=15, kernel_size=3, stride=1, padding=1) # --> output (15, 224, 224)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # --> output (15, 112, 112)\n",
    "\n",
    "        # Classifier\n",
    "        self.fc1 = nn.Linear(in_features=15*112*112, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ],
   "id": "76b42531e412acc8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:36:07.004625Z",
     "start_time": "2025-08-11T08:36:06.947890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Definition\n",
    "model = Model(num_classes=38)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "9bcb92f5f57a6f61",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:42:28.556803Z",
     "start_time": "2025-08-11T08:36:07.116188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch + 1} / {n_epochs}\")\n",
    "\n",
    "    for imgs, labels in progress_bar:\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)"
   ],
   "id": "e604d2623e5a83a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1 / 2:   0%|          | 0/1084 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0e33da5d9c34875abefdf199bbe382b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2 / 2:   0%|          | 0/1084 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8f39f8f079147ea9dbe93809a23529d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:42:28.758659Z",
     "start_time": "2025-08-11T08:42:28.706134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save Model Weights\n",
    "torch.save(model.state_dict(), f\"model.pth\")"
   ],
   "id": "d051fc5d5cf0f541",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Post-Training Dynamic Quantization",
   "id": "16f24913c252b8c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:42:29.042209Z",
     "start_time": "2025-08-11T08:42:29.035516Z"
    }
   },
   "cell_type": "code",
   "source": "import torch.quantization",
   "id": "78fa7c7ac883f5c7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:42:29.155780Z",
     "start_time": "2025-08-11T08:42:29.088540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_fp32 = Model(num_classes=38)\n",
    "model_fp32.load_state_dict(torch.load(\"model.pth\"))\n",
    "model_fp32.eval()"
   ],
   "id": "3d11dd3441e4a513",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=188160, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:42:29.359026Z",
     "start_time": "2025-08-11T08:42:29.225753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# APPLYING DYNAMIC\n",
    "\"\"\"\n",
    "The weights are converted from float32 to int8,\n",
    "Then during inference time, the activations are converted to int8 on the fly inside the layer just before the dot product.\n",
    "The computation (matmul + bias add) is done in int8, then converted back to float32.\n",
    "\"\"\"\n",
    "# Specify the layers to quantize\n",
    "model_int8_dynamic = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,\n",
    "    {torch.nn.Linear}, # A set of layer types to quantize\n",
    "    dtype=torch.qint8 # The target dtype\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "torch.save(model_int8_dynamic.state_dict(), f\"model_int8_dynamic.pth\")\n",
    "\n",
    "# Compare Sizes\n",
    "fp32_size = os.path.getsize('model.pth') / 1e6\n",
    "int8_size = os.path.getsize('model_int8_dynamic.pth') / 1e6\n",
    "print(fp32_size)\n",
    "print(f\"{int8_size}, a {100 - (int8_size/fp32_size) * 100:.2f}% reduction in size\")"
   ],
   "id": "780fc6a2d92c5033",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.609853\n",
      "7.160743, a 74.97% reduction in size\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Post-Training Static Quantization (ONNX)",
   "id": "2a31fe9170ede215"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:42:30.574120Z",
     "start_time": "2025-08-11T08:42:29.437226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import onnx\n",
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import quantize_static, QuantType, CalibrationDataReader\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process\n",
    "\n",
    "class ONNXDataReader(CalibrationDataReader):\n",
    "    def __init__(self, dataloader: DataLoader, onnx_input_name: str):\n",
    "        \"\"\"\n",
    "        Initializes the data reader\n",
    "\n",
    "        :param dataloader: A PyTorch DataLoader providing calibration data\n",
    "        :param onnx_input_name: The name of the input node in the ONNX model\n",
    "        \"\"\"\n",
    "\n",
    "        self.iterator = iter(dataloader)\n",
    "        self.onnx_input_name = onnx_input_name\n",
    "\n",
    "    def get_next(self):\n",
    "        \"\"\"\n",
    "        Returns the next batch of data for calibration\n",
    "        The quantizer will call this method repeatedly\n",
    "        \"\"\"\n",
    "        try:\n",
    "            images, _ = next(self.iterator)\n",
    "            # The quantizer expects a dictionary mapping input names to numpy arrays\n",
    "            return {self.onnx_input_name: images.numpy()}\n",
    "        except StopIteration:\n",
    "            # Returns None to signal the end of the calibration dataset\n",
    "            return None"
   ],
   "id": "a113fa2145b2d03a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:42:30.649Z",
     "start_time": "2025-08-11T08:42:30.641508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Some helper functions\n",
    "def print_onnx_model_size(model_path, label):\n",
    "    \"\"\"Prints the size of the ONNX model file in MB\"\"\"\n",
    "    size = os.path.getsize(model_path) / 1e6\n",
    "    print(f\"Size of {label} ONNX model: {size:.2f} MB\")\n",
    "\n",
    "def benchmark_onnx_inference(model_path, label):\n",
    "    \"\"\"Runs a quick benchmark to measure ONNX model inference latency\"\"\"\n",
    "    # Create an ONNX Runtime inference session\n",
    "    session = onnxruntime.InferenceSession(model_path)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "\n",
    "    # Use a typical image input size\n",
    "    dummy_input = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
    "\n",
    "    # Warm-up runs\n",
    "    for _ in range(10):\n",
    "        _ = session.run(None, {input_name: dummy_input})\n",
    "\n",
    "    # Timed runs\n",
    "    timings = []\n",
    "    for _ in range(100):\n",
    "        start_time = time.time()\n",
    "        _ = session.run(None, {input_name: dummy_input})\n",
    "        end_time = time.time()\n",
    "        timings.append(end_time - start_time)\n",
    "\n",
    "    avg_latency = np.mean(timings) * 1000\n",
    "    print(f\"Average latency for {label} ONNX model: {avg_latency:.3f} ms\")"
   ],
   "id": "3df32dcdcfceac4f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:42:32.391660Z",
     "start_time": "2025-08-11T08:42:30.719406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fp32_onnx_path = \"temp_model_fp32.onnx\"\n",
    "preprocessed_onnx_path = \"temp_model_preprocessed.onnx\"\n",
    "int8_onnx_path = \"temp_model_int8.onnx\"\n",
    "\n",
    "\n",
    "# 1. ARRANGE: Get a trained FP32 model and calibration data\n",
    "fp32_model = Model(num_classes=38)\n",
    "fp32_model.load_state_dict(torch.load(\"model.pth\"))\n",
    "fp32_model.eval()\n",
    "\n",
    "# calibration_dataset = datasets.FakeData(\n",
    "#     size=200, image_size=(3, 224, 224), transform=transforms.ToTensor()\n",
    "# )\n",
    "# calibration_loader = DataLoader(calibration_dataset, batch_size=16)\n",
    "\n",
    "calibration_loader = get_dataloader(data_location=paths.DATA_DIR / \"processed\" / \"train\", img_size=224, batch_size=10)\n",
    "\n",
    "\n",
    "# 2. EXPORT: Convert the PyTorch model to FP32 ONNX format\n",
    "# Create a dummy tensor, required by ONNX exporter to trace the model's computation graph\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "torch.onnx.export(\n",
    "    fp32_model,\n",
    "    (dummy_input, ),\n",
    "    fp32_onnx_path,\n",
    "    opset_version=13,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Call helper function\n",
    "print_onnx_model_size(fp32_onnx_path, \"Original FP32\")\n",
    "\n",
    "# 3. PRE-PROCESS: Run graph optimizations and shape inference\n",
    "quant_pre_process(\n",
    "    input_model_path=fp32_onnx_path,\n",
    "    output_model_path=preprocessed_onnx_path\n",
    ")"
   ],
   "id": "95cd79be27433d44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Original FP32 ONNX model: 28.61 MB\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:43:42.723585Z",
     "start_time": "2025-08-11T08:42:32.469277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. QUANTIZE: Perform Static Quantization on the pre-processed model\n",
    "\n",
    "# a. Create the Data Reader\n",
    "# First, get the input name from the ONNX model graph\n",
    "onnx_model = onnx.load(preprocessed_onnx_path)\n",
    "input_name = onnx_model.graph.input[0].name\n",
    "\n",
    "calibration_data_reader = ONNXDataReader(\n",
    "    dataloader=calibration_loader,\n",
    "    onnx_input_name=input_name\n",
    ")\n",
    "\n",
    "# b. Call the quantization function\n",
    "quantize_static(\n",
    "    model_input=preprocessed_onnx_path,\n",
    "    model_output=int8_onnx_path,\n",
    "    calibration_data_reader=calibration_data_reader,\n",
    "    quant_format='QDQ', # Quantize-Dequantize format\n",
    "    per_channel=True, # Use per-channel quantization for conv layers\n",
    "    weight_type=QuantType.QUInt8,\n",
    "    activation_type=QuantType.QUInt8,\n",
    ")\n",
    "\n",
    "print_onnx_model_size(int8_onnx_path, \"INT8 (Static)\")"
   ],
   "id": "c98195055c2501fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of INT8 (Static) ONNX model: 7.16 MB\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:43:46.351784Z",
     "start_time": "2025-08-11T08:43:45.577956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Benchmark\n",
    "benchmark_onnx_inference(fp32_onnx_path, \"FP32\")\n",
    "benchmark_onnx_inference(int8_onnx_path, \"INT8 (Static)\")"
   ],
   "id": "771b22e6a5994945",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency for FP32 ONNX model: 3.226 ms\n",
      "Average latency for INT8 (Static) ONNX model: 2.828 ms\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:43:49.106415Z",
     "start_time": "2025-08-11T08:43:49.091888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. CLEAN UP\n",
    "print(\"\\n--- 6. Cleaning up temporary files ---\")\n",
    "os.remove(fp32_onnx_path)\n",
    "os.remove(preprocessed_onnx_path)\n",
    "os.remove(int8_onnx_path)\n",
    "print(\"Done.\")"
   ],
   "id": "26f69eaa4a1fad45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Cleaning up temporary files ---\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
